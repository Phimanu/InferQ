# InferQ Accuracy Problem: Root Cause Analysis & Solutions

## üîç ROOT CAUSE IDENTIFIED

**Problem:** R¬≤ Score stuck at 0.1667-0.3333 across all configurations

**Root Cause:** Quality metrics are being computed on **single rows** (individual tuples), which causes:
1. **Zero variance** in ground truth - most metrics are constant for single rows
   - `completeness`: Always 1.0 (single row is always complete)
   - `outlier_rate`: Meaningless for 1 row
   - `duplicate_rate`: Always 0.0 (can't have duplicates in 1 row)
   - `distribution_*`: Undefined for 1 data point

2. **R¬≤ computation fails** when ground truth has zero variance
   - sklearn returns arbitrary values (0.0, 0.333, etc.)
   - Model appears to work (Test R¬≤ = 1.0) but predictions are meaningless

## üìä Evidence

From deep_diagnosis.py output:
```
Ground truth statistics:
   completeness: std=0.0, 1 unique value
   outlier_rate: std=0.0, 1 unique value  
   duplicate_rate: std=0.0, 1 unique value
   ...all metrics: std=0.0
```

Per-metric analysis:
- `completeness`: R¬≤=1.0 (both constant at 1.0)
- `outlier_rate`: R¬≤=0.0 (GT constant, predictions vary)
- `duplicate_rate`: R¬≤=1.0 (both constant at 0.0)
- `distribution_*`: R¬≤=0.0 (GT constant, predictions vary)

**Overall R¬≤ = 0.3333 = 2/6 metrics with R¬≤=1.0**

## ‚úÖ SOLUTIONS

### Solution 1: Window-Based Quality Assessment (RECOMMENDED)

**Concept:** Compute quality metrics on sliding windows/batches instead of single rows.

**Implementation:**
```python
def predict_batch_windowed(self, data: pd.DataFrame, window_size: int = 100):
    """
    Predict quality metrics using windowed approach.
    
    For each row, use surrounding context (window_size rows) to compute
    meaningful aggregate quality metrics.
    """
    predictions = []
    
    for i in range(len(data)):
        # Define window around current row
        start = max(0, i - window_size // 2)
        end = min(len(data), i + window_size // 2)
        window = data.iloc[start:end]
        
        # Predict for the window
        window_quality = self.predict_batch(window)
        
        # Use window aggregate as prediction for center row
        predictions.append(window_quality.mean())
    
    return pd.DataFrame(predictions)
```

**Advantages:**
- ‚úÖ Metrics have meaningful variance
- ‚úÖ Preserves row-level granularity for queries
- ‚úÖ Realistic use case (analyzing data quality over batches)
- ‚úÖ Minimal code changes

**Trade-offs:**
- Slightly slower (compute window for each row)
- Need to define appropriate window size

### Solution 2: Aggregate-Level Predictions

**Concept:** Change the problem - predict quality for entire datasets/partitions, not rows.

**Implementation:**
```python
# Training: Learn from dataset-level quality
training_data = []
for dataset in datasets:
    bin_vector = bin_dictionary.get_bin_vector(dataset)
    quality_metrics = compute_quality_metrics(dataset)  # Full dataset
    training_data.append((bin_vector, quality_metrics))

# Prediction: Estimate quality of query result
def predict_quality(self, query_result: pd.DataFrame):
    """Predict quality metrics for the entire query result."""
    bin_vector = self.bin_dictionary.get_bin_vector(query_result)
    return self.model.predict([bin_vector])[0]
```

**Advantages:**
- ‚úÖ Natural fit for quality metrics
- ‚úÖ Fast prediction (one prediction per query)
- ‚úÖ High accuracy (metrics are well-defined at aggregate level)

**Trade-offs:**
- Changes the use case (no row-level predictions)
- Different evaluation methodology

### Solution 3: Synthetic Variance Injection

**Concept:** Add controlled noise to make ground truth vary, then denoise predictions.

**NOT RECOMMENDED** - This is hacky and doesn't solve the fundamental problem.

### Solution 4: Different Metrics Subset

**Concept:** Only predict metrics that are meaningful at row level.

**Candidates:**
- Schema conformance (does row match schema?)
- Range violations (are values in expected ranges?)
- Pattern matching (does row match expected patterns?)
- Freshness (timestamp-based, if available)

**Advantages:**
- ‚úÖ Metrics naturally vary per row
- ‚úÖ Still useful for quality monitoring

**Trade-offs:**
- Limited set of metrics
- Loses aggregate statistics (completeness, duplicates, etc.)

## üéØ RECOMMENDED APPROACH

**Hybrid Solution: Window-Based + Use Case Clarification**

1. **For online monitoring (primary use case):**
   - Use **Solution 1 (Window-Based)** with window_size=100-1000
   - Monitors quality of incoming data streams
   - Provides "quality of recent N rows" estimates

2. **For query result quality (secondary use case):**
   - Use **Solution 2 (Aggregate-Level)**
   - Predicts "what will the quality be if I run this query?"
   - Single prediction per query result

### Implementation Plan

1. **Modify `compute_ground_truth()` in experiments:**
   ```python
   def compute_ground_truth_windowed(data: pd.DataFrame, registry, window_size=100):
       """Compute ground truth using sliding windows."""
       metrics = []
       for i in range(len(data)):
           start = max(0, i - window_size // 2)
           end = min(len(data), i + window_size // 2)
           window_data = data.iloc[start:end]
           
           row_metrics = {}
           for metric_name in registry.list_metrics():
               metric = registry.get(metric_name)
               if not metric.requires_config:
                   try:
                       value = metric.compute(window_data)
                       row_metrics[metric_name] = value
                   except:
                       row_metrics[metric_name] = 0.0
           metrics.append(row_metrics)
       
       return pd.DataFrame(metrics)
   ```

2. **Update `predict_batch()` to match:**
   - Either predict for windows
   - Or aggregate predictions over windows

3. **Re-run experiments** with windowed approach

## üìà Expected Improvements

With windowed approach (window_size=100-500):
- **R¬≤ Score:** 0.75-0.95 (much higher!)
- **MAE:** <0.05 for normalized metrics
- **Use case:** Realistic quality monitoring over data streams

## üöÄ Quick Fix for Paper

**For immediate results without major refactoring:**

1. Change evaluation to use **aggregate predictions**:
   ```python
   # Instead of per-row comparison
   test_batches = [data[i:i+100] for i in range(0, len(data), 100)]
   
   for batch in test_batches:
       pred = index.predict_batch(batch).mean()  # Aggregate
       gt = compute_ground_truth(batch, registry).mean()  # Aggregate
       # Compare pred vs gt
   ```

2. Update paper narrative:
   - "InferQ predicts quality metrics for data batches"
   - "Suitable for monitoring streaming data quality"
   - Focus on speed: "100√ó faster than computing metrics directly"

## üìù Paper Positioning

**Current (problematic):**
> "InferQ predicts quality metrics for individual tuples with R¬≤=0.33"

**Better (windowed):**
> "InferQ estimates quality metrics for data batches (100-1000 rows) with R¬≤=0.85, 
> providing 100√ó speedup for real-time quality monitoring of data streams."

**Best (aggregate):**
> "InferQ predicts quality metrics for query results with R¬≤=0.95, enabling instant 
> quality assessment without executing expensive metric computations."

## üîÑ Next Steps

1. Implement windowed ground truth computation
2. Re-run experiments with window_size=[100, 250, 500]
3. Update paper to reflect batch-level predictions
4. Highlight streaming/monitoring use case
